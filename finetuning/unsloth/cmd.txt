# IN WSL

conda create --name ft \
    python=3.11 \
    pytorch-cuda=12.4 \
    pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
    -y
conda activate ft

pip install unsloth

pip uninstall xformers -y
pip install xformers --index-url https://download.pytorch.org/whl/cu124

# IN WINDOWS:
cd to this directory, then:

python github_repository_scraper.py https://github.com/sadmann7/shadcn-table --process-all-branches --output-file ../repo_datasets/shadcn-table_dataset.json --stop-at=70051a0c05d03455dd1ad77062006b95f07b8e46 --no-llm-scan

python github_repository_scraper.py https://github.com/vuejs/core --process-all-branches --stop-at=34989ef7fe846df3d0504d2719149bf42103644c --output-file ../repo_datasets/vue_dataset.json --no-llm-scan

--stats-only can be added to analyze repos to modify the script to be more efficient
--no-llm-scan to go FAST (might just be the optimal way to add commits to the dataset)

[ADD THE INSTRUCTIONS HERE TO INSTALL REQUIREMENTS FOR RUNNING THE SCRIPTS]

python combine_repo_datasets.py

run in WSL/linux:
python unsloth-cli.py --model_name "google/gemma-3-1b-it" --max_steps 1600 --per_device_train_batch_size 4 --load_in_4bit --save_model --save_path "testing/data-set-diffchecker-3.8k-max-1028tok-commits-v6" --save_method "lora" 

back to windows (if you are on it):
python ../../../llama.cpp/convert_hf_to_gguf.py "../../../coding_tools/finetuning/unsloth/testing/data-set-diffchecker-3.8k-max-1028tok-commits-v6" --outfile ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf

# Testing regular, non tuned Gemma 1B
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt_in_dataset.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt1.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt2.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt3.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt4.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt5.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt6.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt7.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt8.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt9.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/gemma-3-1b-it-Q4_K_M.gguf -f ../prompt_testing/prompt10.txt --temp 0

# Testing the new model
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt_in_dataset.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt1.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt2.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt3.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt4.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt5.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt6.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt7.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt8.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt9.txt --temp 0
../../../llama.cpp/build/bin/Release/llama-cli -m ../../../models/diffchecker-3.8k-max-1028tok-commits-v6.gguf -f ../prompt_testing/prompt10.txt --temp 0
